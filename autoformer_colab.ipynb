{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install reformer_pytorch\n",
        "!git clone --recursive https://github.com/hjk1996/pm_prediction.git\n",
        "%cd pm_prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_Ml2aEKMJsf",
        "outputId": "4a6a4e81-00a5-4eaf-a8c2-160bde03b69b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting reformer_pytorch\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer_pytorch)\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops (from reformer_pytorch)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting local-attention (from reformer_pytorch)\n",
            "  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n",
            "Collecting product-key-memory (from reformer_pytorch)\n",
            "  Downloading product_key_memory-0.1.10.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from reformer_pytorch) (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->reformer_pytorch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->reformer_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->reformer_pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->reformer_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->reformer_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->reformer_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->reformer_pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->reformer_pytorch) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->reformer_pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->reformer_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: axial-positional-embedding, product-key-memory\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2901 sha256=27991cebd76161f0173c60786925869c7e6a2bddcda03e2d232c978d7421e72f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "  Building wheel for product-key-memory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-py3-none-any.whl size=3068 sha256=8bb65596419bbefe792ccb17b4174f7b154886ebf9abb1ebea8e382dca990b95\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/97/82/f94ef75772952e1eaec19864fc840620ec94d9ac7c9c0b6823\n",
            "Successfully built axial-positional-embedding product-key-memory\n",
            "Installing collected packages: einops, product-key-memory, local-attention, axial-positional-embedding, reformer_pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.6.1 local-attention-1.8.5 product-key-memory-0.1.10 reformer_pytorch-1.4.4\n",
            "Cloning into 'pm_prediction'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 40 (delta 16), reused 34 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 2.85 MiB | 2.66 MiB/s, done.\n",
            "Submodule 'Autoformer' (https://github.com/hjk1996/Autoformer.git) registered for path 'Autoformer'\n",
            "Cloning into '/content/pm_prediction/Autoformer'...\n",
            "remote: Enumerating objects: 348, done.        \n",
            "remote: Counting objects: 100% (175/175), done.        \n",
            "remote: Compressing objects: 100% (65/65), done.        \n",
            "remote: Total 348 (delta 127), reused 136 (delta 110), pack-reused 173        \n",
            "Receiving objects: 100% (348/348), 2.19 MiB | 3.73 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n",
            "Submodule path 'Autoformer': checked out '9b84bb0de39303d182865760841624827fb69a49'\n",
            "/content/pm_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Ebekd1VMHze"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path += [os.path.abspath(\"./Autoformer\")]\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import random\n",
        "import argparse\n",
        "import torch\n",
        "import argparse\n",
        "from Autoformer.data_provider.data_loader import CustomPM\n",
        "from Autoformer.exp.exp_main import Exp_Main\n",
        "import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5kT5FWuM2Gc",
        "outputId": "e98799d3-9486-4a69-a1cf-9992aeeac0b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N4-95fFVMHzg"
      },
      "outputs": [],
      "source": [
        "def get_pm_files(path: str, interpolate=True):\n",
        "    paths = glob(os.path.join(path, '*.csv'))\n",
        "    paths.sort()\n",
        "    dfs = []\n",
        "    for path in paths:\n",
        "        df = pd.read_csv(path)\n",
        "        if interpolate:\n",
        "            df.interpolate(inplace=True)\n",
        "        dfs.append(df)\n",
        "    \n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def get_aws_files(path :str, interpolate=True):\n",
        "    paths = glob(os.path.join(path, '*.csv'))\n",
        "    paths.sort()\n",
        "    dfs = []\n",
        "    for path in paths:\n",
        "        df = pd.read_csv(path)\n",
        "        if interpolate:\n",
        "            df.interpolate(inplace=True)\n",
        "        dfs.append(df)\n",
        "    \n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def get_aws_map(path: str) -> dict:\n",
        "    df = pd.read_csv(path)\n",
        "    aws_map = {}\n",
        "    for row in df.itertuples():\n",
        "        aws_map[row[1]] = (row[2], row[3])\n",
        "    \n",
        "    return aws_map\n",
        "\n",
        "def get_pm_map(path: str) -> dict:\n",
        "    df = pd.read_csv(path)\n",
        "    pm_map = {}\n",
        "    for row in df.itertuples():\n",
        "        pm_map[row[1]] = (row[2], row[3])\n",
        "    \n",
        "    return pm_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lb-3-jleMHzg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def find_n_nearest_points(point_name: str, n: int, aws_map: dict, pm_map:dict) -> list:\n",
        "    point = pm_map[point_name]\n",
        "    distances = []\n",
        "    for key, value in aws_map.items():\n",
        "        distances.append((key, np.linalg.norm(np.array(point) - np.array(value))))\n",
        "    \n",
        "    distances.sort(key=lambda x: x[1])\n",
        "    return distances[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tq6cPOcKMHzh"
      },
      "outputs": [],
      "source": [
        "# 위도 경도 스케일링을 위한 값 (min-max)\n",
        "lat_max = 37.0106\n",
        "lat_min = 36.0625\n",
        "lon_max = 127.4938\n",
        "lon_min = 125.5595\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9fWwXwbUMHzh"
      },
      "outputs": [],
      "source": [
        "def merge_n_nearest(aws: pd.DataFrame, pm: pd.DataFrame, n: int, aws_map: dict, pm_map: dict):\n",
        "    selected_columns = ['연도', '일시', '기온(°C)', '풍향(deg)', '풍속(m/s)', '강수량(mm)', '습도(%)', 'lat', 'lon']\n",
        "    year_map = {0: '2017', 1: '2018', 2: '2019', 3: '2020', 4: '2021'}\n",
        "    pms = []\n",
        "    aws['lat'] = aws['lat'].map(lambda x: (x - lat_min) / (lat_max - lat_min))\n",
        "    aws['lon'] = aws['lon'].map(lambda x: (x - lon_min) / (lon_max - lon_min))\n",
        "    pm['lat'] = pm['lat'].map(lambda x: (x - lat_min) / (lat_max - lat_min))\n",
        "    pm['lon'] = pm['lon'].map(lambda x: (x - lon_min) / (lon_max - lon_min))\n",
        "    for key, _ in pm_map.items():\n",
        "        distances = find_n_nearest_points(key, n, aws_map, pm_map)\n",
        "        sub_pm =  pm[pm['측정소'] == key]\n",
        "        for i, dist in enumerate(distances):\n",
        "            dist_name = dist[0]\n",
        "            sub_aws = aws[aws['지점'] == dist_name]\n",
        "            sub_aws = sub_aws[selected_columns]\n",
        "            sub_pm =  pd.merge(sub_aws, sub_pm, on=['연도', '일시'], how='right', suffixes=(f'_aws{i}', f'_pm{i}'))\n",
        "        \n",
        "        sub_pm['date'] = sub_pm['연도'].map(lambda x: year_map[x]) + \"-\" + sub_pm['일시']\n",
        "        sub_pm['date'] = pd.to_datetime(sub_pm['date'], format='%Y-%m-%d %H:%M')\n",
        "        sub_pm.drop(['연도', '일시'], axis=1, inplace=True)\n",
        "        sub_pm.insert(0, 'date', sub_pm.pop('date'))\n",
        "        sub_pm.bfill(inplace=True)\n",
        "        sub_pm.drop(['측정소'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "        \n",
        "        pms.append(sub_pm)\n",
        "\n",
        "    \n",
        "    return pms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xxhjotJ5MHzh"
      },
      "outputs": [],
      "source": [
        "def add_lat_long_to_pm(df: pd.DataFrame, pm_map: dict):\n",
        "    df['lat'] = df['측정소'].map(lambda x: pm_map[x][0])\n",
        "    df['lon'] = df['측정소'].map(lambda x: pm_map[x][1])\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_lat_long_to_aws(df: pd.DataFrame, aws_map: dict):\n",
        "    df['lat'] = df['지점'].map(lambda x: aws_map[x][0])\n",
        "    df['lon'] = df['지점'].map(lambda x: aws_map[x][1])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T1UprFJRMHzi"
      },
      "outputs": [],
      "source": [
        "def make_train_data(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict, aws_map: dict, k :int= 3,  save_path: str = './dataset/pm_for_train'):\n",
        "    aws = add_lat_long_to_aws(aws, aws_map)\n",
        "    pm = add_lat_long_to_pm(pm, pm_map)\n",
        "    data = merge_n_nearest(aws, pm, k, aws_map, pm_map)\n",
        "    if save_path:\n",
        "      os.makedirs(save_path, exist_ok=True)\n",
        "    for i, df in enumerate(data):\n",
        "        df.to_csv(os.path.join(save_path, f'pm_{i}.csv'), index=False)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UgUQPYPnMHzi"
      },
      "outputs": [],
      "source": [
        "def make_test_data(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict, aws_map: dict, k: int = 3):\n",
        "    chunks = []\n",
        "    aws.dropna(inplace=True)\n",
        "    pm.dropna(inplace=True)\n",
        "    aws = add_lat_long_to_aws(aws, aws_map)\n",
        "    pm = add_lat_long_to_pm(pm, pm_map)\n",
        "    data = merge_n_nearest(aws, pm, k, aws_map, pm_map)\n",
        "    data = [df.iloc[:-48] for df in data]\n",
        "    data = pd.concat(data, ignore_index=True)\n",
        "    cols = list(data.columns)\n",
        "    cols.remove('PM2.5')\n",
        "    cols.remove('date')\n",
        "    data = data[[\"date\"] + cols + ['PM2.5']]\n",
        "\n",
        "    x_data = data[data.columns[1:]]\n",
        "    print(len(data))\n",
        "    x_data = np.array(x_data)\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    for i in range(0, len(x_data), 48):\n",
        "        x = x_data[i:i+48]\n",
        "        stamp = data['date'][i:i+48]\n",
        "        stamp = pd.DataFrame(stamp, columns=['date'])\n",
        "        last_date = stamp['date'].iloc[-1]\n",
        "        test_date = [last_date + datetime.timedelta(hours=x) for x in range(1, 73)]\n",
        "        test_stamp = pd.DataFrame(test_date, columns=['date'])\n",
        "        stamp = pd.concat([stamp, test_stamp], ignore_index=True)\n",
        "        stamp['month'] = stamp.date.apply(lambda row: row.month, 1)\n",
        "        stamp['day'] = stamp.date.apply(lambda row: row.day, 1)\n",
        "        stamp['weekday'] = stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "        stamp['hour'] = stamp.date.apply(lambda row: row.hour, 1)\n",
        "        stamp.drop(['date'], axis=1, inplace=True)\n",
        "        x_mark = stamp.values[:48]\n",
        "        y_mark = stamp.values[48-24:]\n",
        "        dec_inp = torch.zeros(48, x.shape[-1])\n",
        "        dec_inp  = torch.cat([torch.tensor(x[24:, :]), dec_inp], dim=0)\n",
        "\n",
        "\n",
        "\n",
        "        chunks.append((x, x_mark, dec_inp, y_mark))\n",
        "\n",
        "\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-H2O-UfmMHzi"
      },
      "outputs": [],
      "source": [
        "# 대회 데이터 구글 드라이브 pollution/dataset 폴더 안에 배치해야함\n",
        "aws_map = get_aws_map('/content/drive/MyDrive/pollution/dataset/META/awsmap.csv')\n",
        "pm_map = get_pm_map('/content/drive/MyDrive/pollution/dataset/META/pmmap.csv')\n",
        "train_pm = get_pm_files('/content/drive/MyDrive/pollution/dataset/TRAIN', interpolate=True)\n",
        "train_aws = get_aws_files('/content/drive/MyDrive/pollution/dataset/TRAIN_AWS', interpolate=True)\n",
        "test_pm = get_pm_files('/content/drive/MyDrive/pollution/dataset/TEST_INPUT', interpolate=False)\n",
        "test_aws = get_aws_files('/content/drive/MyDrive/pollution/dataset/TEST_AWS', interpolate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTbloyEGMHzj",
        "outputId": "982b89a8-db24-4358-b7ce-d9b60a1c3fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52224\n"
          ]
        }
      ],
      "source": [
        "# 훈련에 사용될 train dataframe은 pm_prediction/dataset/pm_for_train 폴더 안에 저장됨\n",
        "train_data = make_train_data(train_aws, train_pm, pm_map, aws_map)\n",
        "test_data = make_test_data(test_aws, test_pm, pm_map, aws_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N1xefKiEMHzj"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.len = len(data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "r0NlPDWrMHzj"
      },
      "outputs": [],
      "source": [
        "# 리더보드 제출용 정답을 만들기 위한 테스트 데이터\n",
        "test_dataset = TestDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r-VTypNuMHzj"
      },
      "outputs": [],
      "source": [
        "# 입력 차원 확인용 데이터셋\n",
        "dataset = CustomPM(data_path='./dataset/pm_for_train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Lm37Ny12MHzk"
      },
      "outputs": [],
      "source": [
        "class DotDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8gEPKtjVMHzk"
      },
      "outputs": [],
      "source": [
        "# 모델 및 훈련 설정\n",
        "\n",
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n",
        "\n",
        "args = DotDict({\n",
        "    #basic\n",
        "    'is_training': True,\n",
        "    'model_id': 'test',\n",
        "    'model': 'Autoformer',\n",
        "    'data': 'customPM',\n",
        "    'root_path': './dataset',\n",
        "    'data_path': 'pm_for_train',\n",
        "    'features': 'MS',\n",
        "    'target': 'PM2.5',\n",
        "    'freq': 'h',\n",
        "    'checkpoints': './checkpoints',\n",
        "    #forecasting task\n",
        "    'seq_len': 48, # seq_len + label_len = pred_len 이여야함.\n",
        "    'label_len': 24,\n",
        "    'pred_len': 72,\n",
        "    #model config\n",
        "    'bucket_size':4,\n",
        "    'n_hashes': 4,\n",
        "    'enc_in': dataset[0][0].shape[1],\n",
        "    'dec_in': dataset[0][0].shape[1],\n",
        "    'c_out': 1,\n",
        "    'd_model': 512,\n",
        "    'n_heads': 8,\n",
        "    'e_layers': 2,\n",
        "    'd_layers': 2,\n",
        "    'd_ff': 2048,\n",
        "    'moving_avg': 25,\n",
        "    'factor': 1,\n",
        "    'distil': True,\n",
        "    'dropout': 0.05,\n",
        "    'embed': 'timeF',\n",
        "    'activation': 'gelu',\n",
        "    'output_attention': False,\n",
        "    #optimization\n",
        "    'num_workers': 8,\n",
        "    'itr': 1,\n",
        "    'train_epochs': 2, # train_epoch 수\n",
        "    'batch_size': 32,\n",
        "    'patience': 3,\n",
        "    'learning_rate': 0.0001,\n",
        "    'des': 'test',\n",
        "    'loss': 'mse',\n",
        "    'lradj': 'type1',\n",
        "    'use_amp': False,\n",
        "\n",
        "    #GPU\n",
        "    'use_gpu': False,\n",
        "    'gpu': 0,\n",
        "    'use_multi_gpu': False,\n",
        "    'devices': '0,1,2,3',  \n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqV7yi5zMHzk",
        "outputId": "c8fbfeb7-5581-42db-9f8f-1e4869ca67c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use CPU\n"
          ]
        }
      ],
      "source": [
        "#모델\n",
        "\n",
        "Exp = Exp_Main\n",
        "exp = Exp(args)  \n",
        "\n",
        "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
        "    args.model_id,\n",
        "    args.model,\n",
        "    args.data,\n",
        "    args.features,\n",
        "    args.seq_len,\n",
        "    args.label_len,\n",
        "    args.pred_len,\n",
        "    args.d_model,\n",
        "    args.n_heads,\n",
        "    args.e_layers,\n",
        "    args.d_layers,\n",
        "    args.d_ff,\n",
        "    args.factor,\n",
        "    args.embed,\n",
        "    args.distil,\n",
        "    args.des, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djJXLBatMHzk",
        "outputId": "67f10407-b042-412e-d993-b74d806b4122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n",
            "torch.Size([32, 96, 24])\n"
          ]
        }
      ],
      "source": [
        "# 모델 훈련\n",
        "exp.train(setting)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O4LuyeeMHzk"
      },
      "outputs": [],
      "source": [
        "# 훈련된 모델로 테스트 데이터 예측. 결과 데이터프레임으로 저장됨. pm_pollution/results 폴더 안에서 찾을 수 있음\n",
        "exp.predict(setting, test_loader, load=False )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpL06weVMHzk"
      },
      "outputs": [],
      "source": [
        "# indepent variables\n",
        "\n",
        "# year (int): start from 0\n",
        "# datetime (str): 'MM-DD HH:MM' format\n",
        "# point (str): categoricla value, 30 unique values\n",
        "# temp (float): temperature in celsius. range from 0 to 1\n",
        "# degree (float): wind direction in degree. range from 0 to 1\n",
        "# speed (float): wind speed in m/s. range from 0 to 1\n",
        "# rain (float): rain in mm. range from 0 to 1\n",
        "# humidity (float): relative humidity in %. range from 0 to 1\n",
        "# station (str): categorical value, 17 unique values, place where the data was collected\n",
        "\n",
        "# dependent variables\n",
        "\n",
        "# PM2.5 (float): PM2.5 concentration in ug/m3. range from 0 to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmVDM8ySMHzk",
        "outputId": "88e3156b-c8b1-46da-c982-a6c2bcdf59f2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 17 list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# 24544 items in each list\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# total length = \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mxs[\u001b[39m0\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# 17 list\n",
        "# 24544 items in each list\n",
        "# total length = \n",
        "len(dataset.xs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puMdQuuOMHzl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGfyzG4tMHzl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pollution",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}