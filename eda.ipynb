{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path += [os.path.abspath(\"./Autoformer\")]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import argparse\n",
    "from Autoformer.data_provider.data_loader import CustomPM\n",
    "from Autoformer.exp.exp_main import Exp_Main\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pm_files(path: str, interpolate=True):\n",
    "    paths = glob(os.path.join(path, '*.csv'))\n",
    "    paths.sort()\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        if interpolate:\n",
    "            df.interpolate(inplace=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_aws_files(path :str, interpolate=True):\n",
    "    paths = glob(os.path.join(path, '*.csv'))\n",
    "    paths.sort()\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        if interpolate:\n",
    "            df.interpolate(inplace=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_aws_map(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    aws_map = {}\n",
    "    for row in df.itertuples():\n",
    "        aws_map[row[1]] = (row[2], row[3])\n",
    "    \n",
    "    return aws_map\n",
    "\n",
    "def get_pm_map(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    pm_map = {}\n",
    "    for row in df.itertuples():\n",
    "        pm_map[row[1]] = (row[2], row[3])\n",
    "    \n",
    "    return pm_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_nearest_points(point_name: str, n: int, aws_map: dict, pm_map:dict) -> list:\n",
    "    point = pm_map[point_name]\n",
    "    distances = []\n",
    "    for key, value in aws_map.items():\n",
    "        distances.append((key, np.linalg.norm(np.array(point) - np.array(value))))\n",
    "    \n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위도 경도 스케일링을 위한 값 (min-max)\n",
    "lat_max = 37.0106\n",
    "lat_min = 36.0625\n",
    "lon_max = 127.4938\n",
    "lon_min = 125.5595\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict):\n",
    "    selected_columns = ['연도', '일시', '기온(°C)', '풍향(deg)', '풍속(m/s)', '강수량(mm)', '습도(%)', 'lat', 'lon']\n",
    "    year_map = {0: '2017', 1: '2018', 2: '2019', 3: '2020', 4: '2021'}\n",
    "    pms = []\n",
    "    aws['lat'] = aws['lat'].map(lambda x: (x - lat_min) / (lat_max - lat_min))\n",
    "    aws['lon'] = aws['lon'].map(lambda x: (x - lon_min) / (lon_max - lon_min))\n",
    "    pm['lat'] = pm['lat'].map(lambda x: (x - lat_min) / (lat_max - lat_min))\n",
    "    pm['lon'] = pm['lon'].map(lambda x: (x - lon_min) / (lon_max - lon_min))\n",
    "    stations = list(pm_map.keys())\n",
    "    stations.sort()\n",
    "    points =  aws['지점'].unique().tolist()\n",
    "    points.sort()\n",
    "    for key in stations:\n",
    "        sub_pm =  pm[pm['측정소'] == key]\n",
    "        for i, point in enumerate(points):\n",
    "            sub_aws = aws[aws['지점'] == point]\n",
    "            sub_aws = sub_aws[selected_columns]\n",
    "            sub_pm =  pd.merge(sub_aws, sub_pm, on=['연도', '일시'], how='right', suffixes=(f'_aws{i}', f'_pm{i}'))\n",
    "            sub_pm.sort_values(by=['연도', '일시'], inplace=True)\n",
    "        \n",
    "        sub_pm['date'] = sub_pm['연도'].map(lambda x: year_map[x]) + \"-\" + sub_pm['일시']\n",
    "        sub_pm['date'] = pd.to_datetime(sub_pm['date'], format='%Y-%m-%d %H:%M')\n",
    "        sub_pm.drop(['연도', '일시'], axis=1, inplace=True)\n",
    "        sub_pm.insert(0, 'date', sub_pm.pop('date'))\n",
    "        sub_pm.bfill(inplace=True)\n",
    "        sub_pm.drop(['측정소'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        pms.append(sub_pm)\n",
    "\n",
    "    \n",
    "    return pms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lat_long_to_pm(df: pd.DataFrame, pm_map: dict):\n",
    "    df['lat'] = df['측정소'].map(lambda x: pm_map[x][0])\n",
    "    df['lon'] = df['측정소'].map(lambda x: pm_map[x][1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lat_long_to_aws(df: pd.DataFrame, aws_map: dict):\n",
    "    df['lat'] = df['지점'].map(lambda x: aws_map[x][0])\n",
    "    df['lon'] = df['지점'].map(lambda x: aws_map[x][1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict, aws_map: dict, save_path: str = './dataset/pm_for_train'):\n",
    "    aws = add_lat_long_to_aws(aws, aws_map)\n",
    "    pm = add_lat_long_to_pm(pm, pm_map)\n",
    "    data = merge_all(aws, pm, pm_map)\n",
    "    for i, df in enumerate(data):\n",
    "        df.to_csv(os.path.join(save_path, f'pm_{i}.csv'), index=False)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict, aws_map: dict):\n",
    "    chunks = []\n",
    "    aws.dropna(inplace=True)\n",
    "    pm.dropna(inplace=True)\n",
    "    aws = add_lat_long_to_aws(aws, aws_map)\n",
    "    pm = add_lat_long_to_pm(pm, pm_map)\n",
    "    data = merge_all(aws, pm, pm_map)\n",
    "    data = [df.iloc[:-48] for df in data]\n",
    "    data = pd.concat(data, ignore_index=True)\n",
    "    cols = list(data.columns)\n",
    "    cols.remove('PM2.5')\n",
    "    cols.remove('date')\n",
    "    data = data[[\"date\"] + cols + ['PM2.5']]\n",
    "\n",
    "    x_data = data[data.columns[1:]]\n",
    "    print(len(data))\n",
    "    x_data = np.array(x_data)\n",
    "  \n",
    "\n",
    "    for i in range(0, len(x_data), 48):\n",
    "        x = x_data[i:i+48]\n",
    "        stamp = data['date'][i:i+48]\n",
    "        stamp = pd.DataFrame(stamp, columns=['date'])\n",
    "        last_date = stamp['date'].iloc[-1]\n",
    "        test_date = [last_date + datetime.timedelta(hours=x) for x in range(1, 73)]\n",
    "        test_stamp = pd.DataFrame(test_date, columns=['date'])\n",
    "        stamp = pd.concat([stamp, test_stamp], ignore_index=True)\n",
    "        stamp['month'] = stamp.date.apply(lambda row: row.month, 1)\n",
    "        stamp['day'] = stamp.date.apply(lambda row: row.day, 1)\n",
    "        stamp['weekday'] = stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "        stamp['hour'] = stamp.date.apply(lambda row: row.hour, 1)\n",
    "        stamp.drop(['date'], axis=1, inplace=True)\n",
    "        x_mark = stamp.values[:48]\n",
    "        y_mark = stamp.values[48-24:]\n",
    "        dec_inp = torch.zeros(48, x.shape[-1])\n",
    "        dec_inp  = torch.cat([torch.tensor(x[24:, :]), dec_inp], dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        chunks.append((x, x_mark, dec_inp, y_mark))\n",
    "\n",
    "\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_map = get_aws_map('./dataset/META/awsmap.csv')\n",
    "pm_map = get_pm_map('./dataset/META/pmmap.csv')\n",
    "train_pm = get_pm_files('./dataset/TRAIN', interpolate=True)\n",
    "train_aws = get_aws_files('./dataset/TRAIN_AWS', interpolate=True)\n",
    "test_pm = get_pm_files('./dataset/TEST_INPUT', interpolate=False)\n",
    "test_aws = get_aws_files('./dataset/TEST_AWS', interpolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52224\n"
     ]
    }
   ],
   "source": [
    "train_data = make_train_data(train_aws, train_pm, pm_map, aws_map)\n",
    "test_data = make_test_data(test_aws, test_pm, pm_map, aws_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomPM(data_path='./dataset/pm_for_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25118482, 0.01277778, 0.18158568, ..., 0.40549648, 0.80634284,\n",
       "        0.056     ],\n",
       "       [0.2543444 , 0.00972222, 0.1892583 , ..., 0.40549648, 0.80634284,\n",
       "        0.06      ],\n",
       "       [0.25592417, 0.01638889, 0.1892583 , ..., 0.40549648, 0.80634284,\n",
       "        0.068     ],\n",
       "       ...,\n",
       "       [0.37598735, 0.3061111 , 0.09207161, ..., 0.40549648, 0.80634284,\n",
       "        0.332     ],\n",
       "       [0.3728278 , 0.32472223, 0.08951407, ..., 0.40549648, 0.80634284,\n",
       "        0.264     ],\n",
       "       [0.36966825, 0.29777777, 0.10485934, ..., 0.40549648, 0.80634284,\n",
       "        0.268     ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args = DotDict({\n",
    "    #basic\n",
    "    'is_training': True,\n",
    "    'model_id': 'test',\n",
    "    'model': 'Autoformer',\n",
    "    'data': 'customPM',\n",
    "    'root_path': './dataset',\n",
    "    'data_path': 'pm_for_train',\n",
    "    'features': 'MS',\n",
    "    'target': 'PM2.5',\n",
    "    'freq': 'h',\n",
    "    'checkpoints': './checkpoints',\n",
    "    #forecasting task\n",
    "    'seq_len': 48, # seq_len + label_len = pred_len 이여야함.\n",
    "    'label_len': 24,\n",
    "    'pred_len': 72,\n",
    "    #model config\n",
    "    'bucket_size':4,\n",
    "    'n_hashes': 4,\n",
    "    'enc_in': dataset[0][0].shape[1],\n",
    "    'dec_in': dataset[0][0].shape[1],\n",
    "    'c_out': 1,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'd_ff': 2048,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'distil': True,\n",
    "    'dropout': 0.05,\n",
    "    'embed': 'timeF',\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    #optimization\n",
    "    'num_workers': 8,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 2, # train_epoch 수\n",
    "    'batch_size': 32,\n",
    "    'patience': 3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'des': 'test',\n",
    "    'loss': 'mse',\n",
    "    'lradj': 'type1',\n",
    "    'use_amp': False,\n",
    "\n",
    "    #GPU\n",
    "    'use_gpu': False,\n",
    "    'gpu': 0,\n",
    "    'use_multi_gpu': False,\n",
    "    'devices': '0,1,2,3',  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "exp = Exp(args)  \n",
    "\n",
    "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "    args.model_id,\n",
    "    args.model,\n",
    "    args.data,\n",
    "    args.features,\n",
    "    args.seq_len,\n",
    "    args.label_len,\n",
    "    args.pred_len,\n",
    "    args.d_model,\n",
    "    args.n_heads,\n",
    "    args.e_layers,\n",
    "    args.d_layers,\n",
    "    args.d_ff,\n",
    "    args.factor,\n",
    "    args.embed,\n",
    "    args.distil,\n",
    "    args.des, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : test_Autoformer_customPM_ftMS_sl48_ll24_pl72_dm512_nh8_el2_dl2_df2048_fc1_ebtimeF_dtTrue_test_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "exp.train(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 모델로 테스트 데이터 예측. 결과 데이터프레임으로 저장됨.\n",
    "exp.predict(setting, test_loader, load=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indepent variables\n",
    "\n",
    "# year (int): start from 0\n",
    "# datetime (str): 'MM-DD HH:MM' format\n",
    "# point (str): categoricla value, 30 unique values\n",
    "# temp (float): temperature in celsius. range from 0 to 1\n",
    "# degree (float): wind direction in degree. range from 0 to 1\n",
    "# speed (float): wind speed in m/s. range from 0 to 1\n",
    "# rain (float): rain in mm. range from 0 to 1\n",
    "# humidity (float): relative humidity in %. range from 0 to 1\n",
    "# station (str): categorical value, 17 unique values, place where the data was collected\n",
    "\n",
    "# dependent variables\n",
    "\n",
    "# PM2.5 (float): PM2.5 concentration in ug/m3. range from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.model.load_state_dict(torch.load('./checkpoints/checkpoint2.pth', map_location=torch.device('cpu')))\n",
    "exp.predict(setting, test_loader, load=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def merge_answer(answer_format: str, prediction: str) -> pd.DataFrame:\n",
    "    answer_form = pd.read_csv(answer_format)\n",
    "    answer = pd.read_csv(prediction)\n",
    "    answer =answer['0'].to_list()\n",
    "    answer = [max(0, x) for x in answer]\n",
    "    answer_form['PM2.5'] = answer\n",
    "    return answer_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_x_mark, batch_dec_inp, batch_y_mark) in enumerate(test_loader):\n",
    "        batch_x = batch_x.float()\n",
    "        batch_x_mark = batch_x_mark.float()\n",
    "        batch_dec_inp = batch_dec_inp.float()\n",
    "        batch_y_mark = batch_y_mark.float()\n",
    "        outputs = exp.model(batch_x, batch_x_mark, batch_dec_inp, batch_y_mark)\n",
    "        a = outputs[:, :, -1:].squeeze().detach().numpy().tolist()\n",
    "        answers.extend(a)\n",
    "answers = [max(0, a) for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_form = pd.read_csv('./dataset/answer_sample.csv')\n",
    "answer_form['PM2.5'] = answers\n",
    "answer_form.to_csv('./lb4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (16685568) does not match length of index (78336)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m merge_answer(\u001b[39m'\u001b[39;49m\u001b[39m./dataset/answer_sample.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mresults/2023-05-04_21-29-02/real_prediction.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m, in \u001b[0;36mmerge_answer\u001b[0;34m(answer_format, prediction)\u001b[0m\n\u001b[1;32m      5\u001b[0m answer \u001b[39m=\u001b[39manswer[\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m      6\u001b[0m answer \u001b[39m=\u001b[39m [\u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m answer]\n\u001b[0;32m----> 7\u001b[0m answer_form[\u001b[39m'\u001b[39;49m\u001b[39mPM2.5\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m answer\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m answer_form\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pollution/lib/python3.9/site-packages/pandas/core/frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3978\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3979\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3980\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pollution/lib/python3.9/site-packages/pandas/core/frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4172\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4173\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4174\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4176\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4177\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4178\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4179\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4180\u001b[0m     ):\n\u001b[1;32m   4181\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4182\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pollution/lib/python3.9/site-packages/pandas/core/frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4912\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4914\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4915\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4916\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pollution/lib/python3.9/site-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (16685568) does not match length of index (78336)"
     ]
    }
   ],
   "source": [
    "a = merge_answer('./dataset/answer_sample.csv', 'results/2023-05-04_21-29-02/real_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('./lb4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
