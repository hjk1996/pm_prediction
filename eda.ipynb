{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path += [os.path.abspath(\"./Autoformer\")]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import argparse\n",
    "from Autoformer.data_provider.data_loader import CustomPM\n",
    "from Autoformer.exp.exp_main import Exp_Main\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pm_files(path: str, interpolate=True):\n",
    "    paths = glob(os.path.join(path, '*.csv'))\n",
    "    paths.sort()\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        if interpolate:\n",
    "            df.interpolate(inplace=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_aws_files(path :str, interpolate=True):\n",
    "    paths = glob(os.path.join(path, '*.csv'))\n",
    "    paths.sort()\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        if interpolate:\n",
    "            df.interpolate(inplace=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_aws_map(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    aws_map = {}\n",
    "    for row in df.itertuples():\n",
    "        aws_map[row[1]] = (row[2], row[3])\n",
    "    \n",
    "    return aws_map\n",
    "\n",
    "def get_pm_map(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    pm_map = {}\n",
    "    for row in df.itertuples():\n",
    "        pm_map[row[1]] = (row[2], row[3])\n",
    "    \n",
    "    return pm_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_nearest_points(point_name: str, n: int, aws_map: dict, pm_map:dict) -> list:\n",
    "    point = pm_map[point_name]\n",
    "    distances = []\n",
    "    for key, value in aws_map.items():\n",
    "        distances.append((key, np.linalg.norm(np.array(point) - np.array(value))))\n",
    "    \n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위도 경도 스케일링을 위한 값 (min-max)\n",
    "lat_max = 37.0106\n",
    "lat_min = 36.0625\n",
    "lon_max = 127.4938\n",
    "lon_min = 125.5595\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_n_nearest(aws: pd.DataFrame, pm: pd.DataFrame, n: int, aws_map: dict, pm_map: dict):\n",
    "    selected_columns = ['연도', '일시', '기온(°C)', '풍향(deg)', '풍속(m/s)', '강수량(mm)', '습도(%)', 'lat', 'lon']\n",
    "    year_map = {0: '2017', 1: '2018', 2: '2019', 3: '2020', 4: '2021'}\n",
    "    pms = []\n",
    "    aws['lat'] = aws['lat'].map(lambda x: (x - lat_min) / (lat_max - lat_min))\n",
    "    aws['lon'] = aws['lon'].map(lambda x: (x - lon_min) / (lon_max - lon_min))\n",
    "    pm['lat'] = pm['lat'].map(lambda x: (x - lat_min) / (lat_max - lat_min))\n",
    "    pm['lon'] = pm['lon'].map(lambda x: (x - lon_min) / (lon_max - lon_min))\n",
    "    pm_keys = list(pm_map.keys())\n",
    "    pm_keys.sort()\n",
    "    for key in pm_keys:\n",
    "        distances = find_n_nearest_points(key, n, aws_map, pm_map)\n",
    "        sub_pm =  pm[pm['측정소'] == key]\n",
    "        for i, dist in enumerate(distances):\n",
    "            dist_name = dist[0]\n",
    "            sub_aws = aws[aws['지점'] == dist_name]\n",
    "            sub_aws = sub_aws[selected_columns]\n",
    "            sub_pm =  pd.merge(sub_aws, sub_pm, on=['연도', '일시'], how='right', suffixes=(f'_aws{i}', f'_pm{i}'))\n",
    "            sub_pm.sort_values(by=['연도', '일시'], inplace=True)\n",
    "        \n",
    "        sub_pm['date'] = sub_pm['연도'].map(lambda x: year_map[x]) + \"-\" + sub_pm['일시']\n",
    "        sub_pm['date'] = pd.to_datetime(sub_pm['date'], format='%Y-%m-%d %H:%M')\n",
    "        sub_pm.drop(['연도', '일시'], axis=1, inplace=True)\n",
    "        sub_pm.insert(0, 'date', sub_pm.pop('date'))\n",
    "        sub_pm.bfill(inplace=True)\n",
    "        sub_pm.drop(['측정소'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        pms.append(sub_pm)\n",
    "\n",
    "    \n",
    "    return pms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lat_long_to_pm(df: pd.DataFrame, pm_map: dict):\n",
    "    df['lat'] = df['측정소'].map(lambda x: pm_map[x][0])\n",
    "    df['lon'] = df['측정소'].map(lambda x: pm_map[x][1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lat_long_to_aws(df: pd.DataFrame, aws_map: dict):\n",
    "    df['lat'] = df['지점'].map(lambda x: aws_map[x][0])\n",
    "    df['lon'] = df['지점'].map(lambda x: aws_map[x][1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict, aws_map: dict, k :int= 3,  save_path: str = './dataset/pm_for_train'):\n",
    "    aws = add_lat_long_to_aws(aws, aws_map)\n",
    "    pm = add_lat_long_to_pm(pm, pm_map)\n",
    "    data = merge_n_nearest(aws, pm, k, aws_map, pm_map)\n",
    "    for i, df in enumerate(data):\n",
    "        df.to_csv(os.path.join(save_path, f'pm_{i}.csv'), index=False)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(aws: pd.DataFrame, pm: pd.DataFrame, pm_map: dict, aws_map: dict, k: int = 3):\n",
    "    chunks = []\n",
    "    aws.dropna(inplace=True)\n",
    "    pm.dropna(inplace=True)\n",
    "    aws = add_lat_long_to_aws(aws, aws_map)\n",
    "    pm = add_lat_long_to_pm(pm, pm_map)\n",
    "    data = merge_n_nearest(aws, pm, k, aws_map, pm_map)\n",
    "    data = [df.iloc[:-48] for df in data]\n",
    "    data = pd.concat(data, ignore_index=True)\n",
    "    cols = list(data.columns)\n",
    "    cols.remove('PM2.5')\n",
    "    cols.remove('date')\n",
    "    data = data[[\"date\"] + cols + ['PM2.5']]\n",
    "\n",
    "    x_data = data[data.columns[1:]]\n",
    "    print(len(data))\n",
    "    x_data = np.array(x_data)\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    for i in range(0, len(x_data), 48):\n",
    "        x = x_data[i:i+48]\n",
    "        stamp = data['date'][i:i+48]\n",
    "        stamp = pd.DataFrame(stamp, columns=['date'])\n",
    "        last_date = stamp['date'].iloc[-1]\n",
    "        test_date = [last_date + datetime.timedelta(hours=x) for x in range(1, 73)]\n",
    "        test_stamp = pd.DataFrame(test_date, columns=['date'])\n",
    "        stamp = pd.concat([stamp, test_stamp], ignore_index=True)\n",
    "        stamp['month'] = stamp.date.apply(lambda row: row.month, 1)\n",
    "        stamp['day'] = stamp.date.apply(lambda row: row.day, 1)\n",
    "        stamp['weekday'] = stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "        stamp['hour'] = stamp.date.apply(lambda row: row.hour, 1)\n",
    "        stamp.drop(['date'], axis=1, inplace=True)\n",
    "        x_mark = stamp.values[:48]\n",
    "        y_mark = stamp.values[48-24:]\n",
    "        dec_inp = torch.zeros(48, x.shape[-1])\n",
    "        dec_inp  = torch.cat([torch.tensor(x[24:, :]), dec_inp], dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        chunks.append((x, x_mark, dec_inp, y_mark))\n",
    "\n",
    "\n",
    "\n",
    "    return chunks, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_map = get_aws_map('./dataset/META/awsmap.csv')\n",
    "pm_map = get_pm_map('./dataset/META/pmmap.csv')\n",
    "train_pm = get_pm_files('./dataset/TRAIN', interpolate=True)\n",
    "train_aws = get_aws_files('./dataset/TRAIN_AWS', interpolate=True)\n",
    "test_pm = get_pm_files('./dataset/TEST_INPUT', interpolate=False)\n",
    "test_aws = get_aws_files('./dataset/TEST_AWS', interpolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52224\n"
     ]
    }
   ],
   "source": [
    "train_data = make_train_data(train_aws, train_pm, pm_map, aws_map)\n",
    "test_data, data = make_test_data(test_aws, test_pm, pm_map, aws_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomPM(data_path='./dataset/pm_for_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args = DotDict({\n",
    "    #basic\n",
    "    'is_training': True,\n",
    "    'model_id': 'test',\n",
    "    'model': 'Autoformer',\n",
    "    'data': 'customPM',\n",
    "    'root_path': './dataset',\n",
    "    'data_path': 'pm_for_train',\n",
    "    'features': 'MS',\n",
    "    'target': 'PM2.5',\n",
    "    'freq': 'h',\n",
    "    'checkpoints': './checkpoints',\n",
    "    #forecasting task\n",
    "    'seq_len': 48, # seq_len + label_len = pred_len 이여야함.\n",
    "    'label_len': 24,\n",
    "    'pred_len': 72,\n",
    "    #model config\n",
    "    'bucket_size':4,\n",
    "    'n_hashes': 4,\n",
    "    'enc_in': dataset[0][0].shape[1],\n",
    "    'dec_in': dataset[0][0].shape[1],\n",
    "    'c_out': 1,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'd_ff': 2048,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'distil': True,\n",
    "    'dropout': 0.05,\n",
    "    'embed': 'timeF',\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    #optimization\n",
    "    'num_workers': 8,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 2, # train_epoch 수\n",
    "    'batch_size': 32,\n",
    "    'patience': 3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'des': 'test',\n",
    "    'loss': 'mse',\n",
    "    'lradj': 'type1',\n",
    "    'use_amp': False,\n",
    "\n",
    "    #GPU\n",
    "    'use_gpu': False,\n",
    "    'gpu': 0,\n",
    "    'use_multi_gpu': False,\n",
    "    'devices': '0,1,2,3',  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "exp = Exp(args)  \n",
    "\n",
    "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "    args.model_id,\n",
    "    args.model,\n",
    "    args.data,\n",
    "    args.features,\n",
    "    args.seq_len,\n",
    "    args.label_len,\n",
    "    args.pred_len,\n",
    "    args.d_model,\n",
    "    args.n_heads,\n",
    "    args.e_layers,\n",
    "    args.d_layers,\n",
    "    args.d_ff,\n",
    "    args.factor,\n",
    "    args.embed,\n",
    "    args.distil,\n",
    "    args.des, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : test_Autoformer_customPM_ftMS_sl48_ll24_pl72_dm512_nh8_el2_dl2_df2048_fc1_ebtimeF_dtTrue_test_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n",
      "torch.Size([8, 96, 24])\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "exp.train(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 모델로 테스트 데이터 예측. 결과 데이터프레임으로 저장됨.\n",
    "exp.predict(setting, test_loader, load=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indepent variables\n",
    "\n",
    "# year (int): start from 0\n",
    "# datetime (str): 'MM-DD HH:MM' format\n",
    "# point (str): categoricla value, 30 unique values\n",
    "# temp (float): temperature in celsius. range from 0 to 1\n",
    "# degree (float): wind direction in degree. range from 0 to 1\n",
    "# speed (float): wind speed in m/s. range from 0 to 1\n",
    "# rain (float): rain in mm. range from 0 to 1\n",
    "# humidity (float): relative humidity in %. range from 0 to 1\n",
    "# station (str): categorical value, 17 unique values, place where the data was collected\n",
    "\n",
    "# dependent variables\n",
    "\n",
    "# PM2.5 (float): PM2.5 concentration in ug/m3. range from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.model.load_state_dict(torch.load('./checkpoints/checkpoint.pth', map_location=torch.device('cpu')))\n",
    "exp.predict(setting, test_loader, load=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def merge_answer(answer_format: str, prediction: str) -> pd.DataFrame:\n",
    "    answer_form = pd.read_csv(answer_format)\n",
    "    answer = pd.read_csv(prediction)\n",
    "    answer =answer['0'].to_list()\n",
    "    answer = [max(0, x) for x in answer]\n",
    "    answer_form['PM2.5'] = answer\n",
    "    return answer_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = merge_answer('./dataset/answer_sample.csv', 'results/2023-05-02_23-36-24/real_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('./lb3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
