{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path += [os.path.abspath(\"./Autoformer\")]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import argparse\n",
    "from Autoformer.data_provider.data_loader import CustomPM\n",
    "from Autoformer.exp.exp_main import Exp_Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_map = {0: '2016', 1: '2017', 2: '2018', 3: '2019', 4: '2020'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pm_files():\n",
    "    paths = glob('./dataset/TRAIN/*.csv')\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df.interpolate(inplace=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_aws_files():\n",
    "    paths = glob('./dataset/TRAIN_AWS/*.csv')\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df.interpolate(inplace=True)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_aws_map(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    aws_map = {}\n",
    "    for row in df.itertuples():\n",
    "        aws_map[row[1]] = (row[2], row[3])\n",
    "    \n",
    "    return aws_map\n",
    "\n",
    "def get_pm_map(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "    pm_map = {}\n",
    "    for row in df.itertuples():\n",
    "        pm_map[row[1]] = (row[2], row[3])\n",
    "    \n",
    "    return pm_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_nearest_points(point_name: str, n: int, aws_map: dict, pm_map:dict) -> list:\n",
    "    point = pm_map[point_name]\n",
    "    distances = []\n",
    "    for key, value in aws_map.items():\n",
    "        distances.append((key, np.linalg.norm(np.array(point) - np.array(value))))\n",
    "    \n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_n_nearest(aws: pd.DataFrame, pm: pd.DataFrame, n: int, aws_map: dict, pm_map: dict):\n",
    "    selected_columns = ['연도', '일시', '기온(°C)', '풍향(deg)', '풍속(m/s)', '강수량(mm)', '습도(%)', 'lat', 'lon']\n",
    "    year_map = {0: '2017', 1: '2018', 2: '2019', 3: '2020'}\n",
    "    pms = []\n",
    "    for key, _ in pm_map.items():\n",
    "        distances = find_n_nearest_points(key, n, aws_map, pm_map)\n",
    "        sub_pm =  pm[pm['측정소'] == key]\n",
    "        for i, dist in enumerate(distances):\n",
    "            dist_name = dist[0]\n",
    "            sub_aws = aws[aws['지점'] == dist_name]\n",
    "            sub_aws = sub_aws[selected_columns]\n",
    "            sub_pm =  pd.merge(sub_aws, sub_pm, on=['연도', '일시'], how='right', suffixes=(f'_aws{i}', f'_pm{i}'))\n",
    "        \n",
    "        sub_pm['date'] = sub_pm['연도'].map(lambda x: year_map[x]) + \"-\" + sub_pm['일시']\n",
    "        sub_pm['date'] = pd.to_datetime(sub_pm['date'], format='%Y-%m-%d %H:%M')\n",
    "        sub_pm.drop(['연도', '일시'], axis=1, inplace=True)\n",
    "        sub_pm.insert(0, 'date', sub_pm.pop('date'))\n",
    "        sub_pm.bfill(inplace=True)\n",
    "\n",
    "        \n",
    "        pms.append(sub_pm)\n",
    "    \n",
    "    return pms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws = get_aws_files()\n",
    "# pm = get_pm_files()\n",
    "# aws_map = get_aws_map(\"./dataset/META/awsmap.csv\")\n",
    "# pm_map = get_pm_map(\"./dataset/META/pmmap.csv\")\n",
    "# aws['lat'] = aws['지점'].apply(lambda x: aws_map[x][0])\n",
    "# aws['lon'] = aws['지점'].apply(lambda x: aws_map[x][1])\n",
    "# pm['lat'] = pm['측정소'].apply(lambda x: pm_map[x][0])\n",
    "# pm['lon'] = pm['측정소'].apply(lambda x: pm_map[x][1])\n",
    "# x_plus = y[['연도', '일시', '측정소']]\n",
    "# x = pd.merge(x, x_plus, on=['연도', '일시'])\n",
    "# y = y['PM2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pms = merge_n_nearest(aws, pm, 3, aws_map, pm_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, pm in enumerate(pms):\n",
    "#     pm.to_csv(f'./dataset/pm_for_train/pm_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomPM(data_path='./dataset/pm_for_train', flag='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[-1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10449"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.list_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175610"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args = DotDict({\n",
    "    #basic\n",
    "    'is_training': True,\n",
    "    'model_id': 'test',\n",
    "    'model': 'Autoformer',\n",
    "    'data': 'customPM',\n",
    "    'root_path': './dataset',\n",
    "    'data_path': 'pm_for_train',\n",
    "    'features': 'MS',\n",
    "    'target': 'PM2.5',\n",
    "    'freq': 'h',\n",
    "    'checkpoints': './checkpoints',\n",
    "\n",
    "    #forecasting task\n",
    "    'seq_len': 48, # seq_len + label_len = pred_len 이여야함.\n",
    "    'label_len': 24,\n",
    "    'pred_len': 72,\n",
    "\n",
    "    #model config\n",
    "    'bucket_size':4,\n",
    "    'n_hashes': 4,\n",
    "    'enc_in': dataset[0][0].shape[1],\n",
    "    'dec_in': dataset[0][0].shape[1],\n",
    "    'c_out': 1,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'd_ff': 2048,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'distil': True,\n",
    "    'dropout': 0.05,\n",
    "    'embed': 'timeF',\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "\n",
    "    \n",
    "\n",
    "    #optimization\n",
    "    'num_workers': 8,\n",
    "    'itr': 2,\n",
    "    'train_epochs': 100,\n",
    "    'batch_size': 8,\n",
    "    'patience': 3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'des': 'test',\n",
    "    'loss': 'mse',\n",
    "    'lradj': 'type1',\n",
    "    'use_amp': False,\n",
    "\n",
    "    #GPU\n",
    "    'use_gpu': False,\n",
    "    'gpu': 0,\n",
    "    'use_multi_gpu': False,\n",
    "    'devices': '0,1,2,3',\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 24)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Autoformer.layers.Embed import DataEmbedding, DataEmbedding_wo_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : test_Autoformer_customPM_ftMS_sl48_ll24_pl72_dm512_nh8_el2_dl2_df2048_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.3080468\n",
      "\tspeed: 0.3566s/iter; left time: 1841772.5957s\n",
      "\titers: 200, epoch: 1 | loss: 0.2709801\n",
      "\tspeed: 0.2714s/iter; left time: 1401883.1241s\n",
      "\titers: 300, epoch: 1 | loss: 0.2229000\n",
      "\tspeed: 0.2653s/iter; left time: 1370396.9569s\n",
      "\titers: 400, epoch: 1 | loss: 0.1909914\n",
      "\tspeed: 0.2649s/iter; left time: 1368009.9503s\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        if args.do_predict:\n",
    "            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.predict(setting, True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
    "                                                                                                    args.model,\n",
    "                                                                                                    args.data,\n",
    "                                                                                                    args.features,\n",
    "                                                                                                    args.seq_len,\n",
    "                                                                                                    args.label_len,\n",
    "                                                                                                    args.pred_len,\n",
    "                                                                                                    args.d_model,\n",
    "                                                                                                    args.n_heads,\n",
    "                                                                                                    args.e_layers,\n",
    "                                                                                                    args.d_layers,\n",
    "                                                                                                    args.d_ff,\n",
    "                                                                                                    args.factor,\n",
    "                                                                                                    args.embed,\n",
    "                                                                                                    args.distil,\n",
    "                                                                                                    args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indepent variables\n",
    "\n",
    "# year (int): start from 0\n",
    "# datetime (str): 'MM-DD HH:MM' format\n",
    "# point (str): categoricla value, 30 unique values\n",
    "# temp (float): temperature in celsius. range from 0 to 1\n",
    "# degree (float): wind direction in degree. range from 0 to 1\n",
    "# speed (float): wind speed in m/s. range from 0 to 1\n",
    "# rain (float): rain in mm. range from 0 to 1\n",
    "# humidity (float): relative humidity in %. range from 0 to 1\n",
    "# station (str): categorical value, 17 unique values, place where the data was collected\n",
    "\n",
    "# dependent variables\n",
    "\n",
    "# PM2.5 (float): PM2.5 concentration in ug/m3. range from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 17 list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# 24544 items in each list\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# total length = \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mxs[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 17 list\n",
    "# 24544 items in each list\n",
    "# total length = \n",
    "len(dataset.xs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
